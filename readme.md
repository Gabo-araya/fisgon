# Fisg√≥n - Web crawler de metadatos.

_Web crawler de metadatos con sistema de login simple con funcionalidad de verificaci√≥n de usuarios._

Estas instrucciones te permitir√°n obtener una copia del proyecto en funcionamiento en tu m√°quina local para prop√≥sitos de desarrollo y pruebas.

## Pre-requisitos

_Esta es una lista de los paquetes que deben estar instalados previamente:_

* Python 3
	- Lenguaje de programaci√≥n
	- [Ayuda - https://docs.microsoft.com/en-us/windows/python/beginners)](https://docs.microsoft.com/en-us/windows/python/beginners)
	- [Curso Django desde Cero en youtube](https://www.youtube.com/watch?v=vo4VF3neyrs)

* Pip
	- Gestor de instalaci√≥n de paquetes PIP
	- [Ayuda - https://tecnonucleous.com/2018/01/28/como-instalar-pip-para-python-en-windows-mac-y-linux/](https://tecnonucleous.com/2018/01/28/como-instalar-pip-para-python-en-windows-mac-y-linux/)

* Virtualenv
	- Creador de entornos virtuales para Python
	- [Ayuda - https://techexpert.tips/es/windows-es/instalacion-del-entorno-virtual-de-python-en-windows/](https://techexpert.tips/es/windows-es/instalacion-del-entorno-virtual-de-python-en-windows/)

### Instalaci√≥n pre-requisitos (Windows) üîß

Muchas veces tenemos ese problema com√∫n de no poder instalar ciertas librer√≠as o realizar configuraciones para poder desarrollar en Windows para Web y es por ello que en √©ste tutorial vamos a ver los pasos para instalar Python y configurarlo con Pip y Virtualenv para as√≠ poder empezar a desarrollar aplicaciones basadas en √©ste lenguaje e instalar Django para crear aplicaciones web. [Ver video -> **https://www.youtube.com/watch?v=sG7Q-r_SZhA**](https://www.youtube.com/watch?v=sG7Q-r_SZhA)

1. Descargamos e instalamos Python 3.6 (o una versi√≥n superior) para Windows
	- [https://www.python.org/downloads/](https://www.python.org/downloads/)

2. Agregaremos Python a las variables de entorno de nuestro sistema si es que no se agregaron durante la instalaci√≥n para que as√≠ podamos ejecutarlo desde la terminal `/cmd`
	- `C:\Python34 y C:\Python34\Scripts`

3. Ejecutamos Pip para verificar que est√© instalado correctamente y tambi√©n la versi√≥n
	- `pip --version`

4. Instalamos Virtualenv con
	- `pip install virtualenv`

5. Verificamos la versi√≥n de Virtualenv
	- `virtualenv --version`

6. Crearemos un entorno virtual con Python
	- `virtualenv test`

7. Iniciamos el entorno virtual
	- `.\test\Scripts\activate`

8. Finalmente desactivamos el entorno virtual
	- `deactivate`

## Instalaci√≥n pre-requisitos (GNU/Linux) üîß

1. Ejecutamos Pip para verificar que est√© instalado correctamente
	- `pip3 --version`

2. Instalamos Virtualenv con pip
	- `pip3 install virtualenv`

3. Verificamos la versi√≥n de Virtualenv
	- `virtualenv --version`

4. Crearemos un entorno virtual con Python
	- `python3 -m venv /home/gabo/envs/gaboaraya/env`

5. Activamos el entorno virtual
	- `source /home/gabo/envs/gaboaraya/env/bin/activate`

6. Finalmente desactivamos el entorno virtual
	- `deactivate`

### Instalaci√≥n Local

Seguir los siguientes pasos para la instalaci√≥n local.

1. Clonar el repositorio o subir/descargar los archivos.

	- `git clone https://github.com/Gabo-araya/fisgon-web-crawler.git`

2. Instalar los requerimientos.

	- `python3 -m pip install -r requirements.txt`

3. Revisar settings.py y .env
	- Revisar que la secci√≥n de base de datos est√© configurada para que trabaje con la base de datos SQLite en local.

4. Realizar migraciones
	- Crear archivos de migraci√≥n: `python3 manage.py makemigrations`
	- Realizar migraciones`python3 manage.py migrate`

5. Crear superusuario
	- `python3 manage.py createsuperuser`
	- Si se usa Cpanel es necesario indicar el encoding primero v√≠a terminal:
		-`export PYTHONIOENCODING="UTF-8"; python3.6 manage.py createsuperuser`

6. Obtener archivos est√°ticos
	- `python3 manage.py collectstatic`

7. Iniciar el servidor
	- `python3 manage.py runserver`
	- Iniciar en un puerto espec√≠fico (8000):`python3 manage.py runserver 8000`




### Ejemplo de archivo `.env`

```
# .env
DEBUG=True

SECRET_KEY=clave-secreta-aqui

DATABASE_URL=sqlite:///db.sqlite3
REDIS_URL=redis://localhost:6379/0
ALLOWED_HOSTS=localhost,127.0.0.1

# Configuraci√≥n de Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Configuraci√≥n del Crawler
CRAWLER_USER_AGENT=FisgonCrawler/1.0
CRAWLER_MAX_DEPTH=5
CRAWLER_RATE_LIMIT=1.0

# Configuraci√≥n de la base de datos (puedes ponerla aqu√≠ o en un solo string)
# DB_NAME=fisgon_db
# DB_USER=django_user
# DB_PASSWORD=una_contrase√±a_segura
# DB_HOST=localhost
# DB_PORT=5432

```


### Crear Usuarios y Grupos
```bash
# Abrir shell de Django
python manage.py shell
```

```
# Dentro del shell de Django:
from django.contrib.auth.models import User, Group

# Crear grupos
admin_group = Group.objects.create(name='admin')
crawler_group = Group.objects.create(name='crawler')
viewer_group = Group.objects.create(name='viewer')

# Crear usuarios de prueba
# Usuario crawler
crawler_user = User.objects.create_user(
    username='crawler',
    email='crawler@example.com',
    password='asdf.1234'
)
crawler_user.groups.add(crawler_group)

# Usuario viewer
viewer_user = User.objects.create_user(
    username='viewer',
    email='viewer@example.com',
    password='asdf.1234'
)
viewer_user.groups.add(viewer_group)

# Asignar el superusuario al grupo admin
admin_user = User.objects.get(username='admin')  # Reemplazar con tu username
admin_user.groups.add(admin_group)

exit()
```



### Recolectar Archivos Est√°ticos

```
# Crear directorio static si no existe
mkdir -p staticfiles

# Recolectar archivos est√°ticos
python manage.py collectstatic --noinput
```






## Flujo de Trabajo Completo

### **1. Proceso de Crawling (Descubrimiento)**

```
Usuario configura target ‚Üí Crawler inicia ‚Üí Descubre URLs ‚Üí Filtra por tipo ‚Üí Queue de procesamiento
```

**Detalles del proceso:**

1. **Configuraci√≥n inicial**:
   - Usuario define dominio objetivo (ej: `example.com`)
   - Establece profundidad m√°xima (niveles de recursi√≥n)
   - Define tipos de archivo de inter√©s
   - Configura rate limiting (requests/segundo)

2. **Descubrimiento de URLs**:
   - Inicia con robots.txt y sitemap.xml
   - Rastrea links HTML recursivamente
   - Identifica archivos por extensi√≥n
   - Usa t√©cnicas como directory bruteforcing suave
   - Almacena URLs encontradas en cola Redis

3. **Filtrado inteligente**:
   - Elimina duplicados
   - Filtra por tama√±o de archivo (evita archivos gigantes)
   - Prioriza por tipo de archivo (PDFs > Im√°genes > Multimedia)
   - Respeta blacklists y whitelist definidas

### **2. Proceso de Extracci√≥n (Harvesting)**

```
URL ‚Üí Descarga ‚Üí Identificaci√≥n de tipo ‚Üí Extractor espec√≠fico ‚Üí Metadatos estructurados
```

**Por cada archivo encontrado:**

1. **Descarga controlada**:
   - Verificaci√≥n de Content-Type
   - L√≠mite de tama√±o (ej: m√°ximo 50MB)
   - Timeout configurable
   - Manejo de errores HTTP

2. **Identificaci√≥n precisa**:
   - Magic numbers (primeros bytes del archivo)
   - Extensi√≥n vs contenido real
   - Validaci√≥n de formato

3. **Extracci√≥n especializada**:

   **Para PDFs:**
   ```
   Archivo PDF ‚Üí PyPDF2/pdfplumber ‚Üí
   {
     'author': 'Juan P√©rez',
     'creator': 'Microsoft Word',
     'creation_date': '2023-05-15',
     'modification_date': '2023-05-16',
     'title': 'Informe Confidencial',
     'subject': 'An√°lisis Financiero Q2',
     'producer': 'Adobe Acrobat',
     'pdf_version': '1.4'
   }
   ```

   **Para Im√°genes:**
   ```
   Imagen JPEG ‚Üí ExifRead/Pillow ‚Üí
   {
     'camera_make': 'Canon',
     'camera_model': 'EOS R5',
     'datetime_original': '2023-05-15 14:30:22',
     'gps_coordinates': '(lat: -33.4489, lon: -70.6693)',
     'software': 'Adobe Lightroom 6.0',
     'artist': 'Fot√≥grafo Profesional',
     'copyright': '¬© Empresa XYZ 2023'
   }
   ```

   **Para Office:**
   ```
   Word Doc ‚Üí python-docx ‚Üí
   {
     'author': 'Maria Gonz√°lez',
     'last_modified_by': 'Pedro Silva',
     'created': '2023-05-10',
     'modified': '2023-05-15',
     'company': 'Empresa Confidencial SA',
     'manager': 'Director TI',
     'revision': '5',
     'template': 'plantilla_corporativa.dotx'
   }
   ```

### **3. Proceso de An√°lisis (Intelligence)**

```
Metadatos ‚Üí Normalizaci√≥n ‚Üí An√°lisis de patrones ‚Üí Scoring de riesgo ‚Üí Alertas
```

**An√°lisis multinivel:**

1. **Normalizaci√≥n de datos**:
   - Estandarizaci√≥n de fechas
   - Limpieza de nombres (quitar acentos, may√∫sculas)
   - Categorizaci√≥n de software
   - Geocodificaci√≥n de coordenadas GPS

2. **Detecci√≥n de patrones**:

   **Usuarios frecuentes:**
   ```python
   # Ejemplo de patr√≥n detectado
   {
     'pattern_type': 'frequent_author',
     'value': 'juan.perez@empresa.com',
     'occurrences': 47,
     'risk_score': 8.5,
     'file_types': ['pdf', 'docx', 'xlsx'],
     'date_range': '2023-01-15 to 2023-05-20'
   }
   ```

   **Software desactualizado:**
   ```python
   {
     'pattern_type': 'outdated_software',
     'software': 'Adobe Acrobat 9.0',
     'version_year': 2008,
     'risk_score': 9.2,
     'vulnerability_count': 23,
     'affected_files': 12
   }
   ```

   **Informaci√≥n geogr√°fica:**
   ```python
   {
     'pattern_type': 'location_exposure',
     'coordinates': [(-33.4489, -70.6693), (-33.4512, -70.6580)],
     'location_cluster': 'Santiago Centro, Chile',
     'risk_score': 7.8,
     'files_with_gps': 8
   }
   ```

### **4. Proceso de Almacenamiento (Persistencia)**

```
Metadatos ‚Üí Validaci√≥n ‚Üí Base de datos ‚Üí Indexaci√≥n ‚Üí API de consulta
```

**Estructura de datos propuesta:**

```sql
-- Tabla principal de crawling sessions
crawl_sessions (
    id, user_id, target_domain, status,
    started_at, completed_at, total_files_found
)

-- URLs descubiertas
discovered_urls (
    id, session_id, url, file_type, file_size,
    status, discovered_at, processed_at
)

-- Metadatos extra√≠dos
extracted_metadata (
    id, url_id, metadata_type, field_name,
    field_value, confidence_score, extracted_at
)

-- Patrones detectados
detected_patterns (
    id, session_id, pattern_type, description,
    risk_score, affected_files_count, details_json
)
```

### **5. Proceso de Visualizaci√≥n (Dashboard)**

```
Datos ‚Üí Agregaci√≥n ‚Üí APIs ‚Üí Frontend ‚Üí Visualizaciones interactivas
```

**Tipos de visualizaci√≥n:**

1. **Dashboard principal**:
   - M√©tricas en tiempo real (archivos procesados, patrones detectados)
   - Timeline de actividad del crawl
   - Top 10 autores m√°s frecuentes
   - Mapa de calor de tipos de archivo

2. **An√°lisis detallado**:
   - Gr√°fico de red de relaciones autor-documento
   - Timeline de versiones de software
   - Mapa geogr√°fico de coordenadas GPS
   - An√°lisis de riesgo por categor√≠a

3. **Reportes exportables**:
   - PDF ejecutivo con hallazgos principales
   - CSV detallado para an√°lisis t√©cnico
   - Recomendaciones de mitigaci√≥n




### Funcionalidades activas

- Creaci√≥n de usuario (User)
- Completitud de campos asociados de usuario (Onboarding)
- Modificaci√≥n de settings
- Modificaci√≥n de correo electr√≥nico
- Validaci√≥n de correo electr√≥nico
- Eliminaci√≥n de usuario
- Inicio de sesi√≥n de usuario
- Cierre de sesi√≥n de usuario




## Herramientas de construcci√≥n

_Estas son las herramientas que se han utilizado en este proyecto_

* [Django](https://www.djangoproject.com/) - El framework web usado

### Acceso a secci√≥n de administraci√≥n de Django

- [http://localhost:8000/admin/](http://localhost:8000/admin/)

- Usuario: `admin4`
- Password: `asdf.1234`
- Tipo: `admin`

- Usuario: `gabo`
- Password: `asdf.1234`
- Tipo: `admin`

- Usuario: `crawler`
- Password: `asdf.1234`
- Tipo: `crawler`

- Usuario: `viewer`
- Password: `asdf.1234`
- Tipo: `viewer`


## Autor ‚úíÔ∏è

* **[Gabo Araya](https://github.com/Gabo-araya/)** - *Backend y documentaci√≥n*
